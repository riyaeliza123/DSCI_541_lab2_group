{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 541 Lab 2 - Privacy\n",
    "\n",
    "## Required readings\n",
    "\n",
    "We covered some of these videos in lab, but including them here so that you have everything in one place. There might be questions on specifics from the required readings whereas the optional readings are more of a general help, optional questions, and if you are interested to explore further.\n",
    "\n",
    "- **Lec 3** Madhumita Murgia [\"How data brokers sold my identity\"](https://www.youtube.com/watch?v=AU66C6HePfg) (0:00 - 9:40)\n",
    "- **Lec 3** Michal Kosinski [\"Part One: The End of Privacy,  Data Scientists Know All Your Secrets\"](https://www.youtube.com/watch?v=X9jVjCVOUIM) (13 min video)\n",
    "- **Lec 3** Capture Behavioral Engagement [\"Marketing Automation for Higher Education\"](https://www.youtube.com/watch?v=MjTZM7VQDzQ) (2 min video)\n",
    "- **Lec 4** Latanya Sweeney [\"Data Privacy in the Digital Age\"](https://youtu.be/Pn4p4VgSyCs) (0:00 - 16:40)\n",
    "- Sara Buhr [\"An Amazon Echo may be the key to solving a murder case\"](https://techcrunch.com/2016/12/27/an-amazon-echo-may-be-the-key-to-solving-a-murder-case/) (3 min read)\n",
    "- Emma Wollacott [\"70,000 OkCupid Profiles Leaked, Intimate Details And All\"](https://www.forbes.com/sites/emmawoollacott/2016/05/13/intimate-data-of-70000-okcupid-users-released/) (4 min read)\n",
    "- CPAC New clip [\"Clearview AI violated Canada's federal and provincial laws\"](https://www.youtube.com/watch?v=czNTHe1rex8) (5 min video)\n",
    "\n",
    "## Optional readings\n",
    "\n",
    "<details><summary>Click to show</summary>\n",
    "\n",
    "You don't have to read these before lab and you don't have to read all of them. The main reason for including them here is as a recommendation of where to deepen your knowledge if there is something that you find particularly interesting. I am giving a brief background to each so you can pick the ones that peak your interest.\n",
    "\n",
    "- Minute Physics [Protecting Privacy with MATH](https://www.youtube.com/watch?v=pT19VwBAqKA)\n",
    "    - Made when the US census switched to using differential privacy, includes more examples and intuition behind the concepts.\n",
    "- Damien Desfontaines [\"Differential privacy in (a bit) more detail\"](https://desfontain.es/privacy/differential-privacy-in-more-detail.html)\n",
    "    - A more technical explanation of differential privacy, including converting the coin flip example into the equations used when assessing privacy loss.\n",
    "- Statistics Canada [\"A Brief Survey of Privacy Preserving Technologies\"](https://www.statcan.gc.ca/en/data-science/network/privacy-preserving)\n",
    "    - Techniques that Canadian government is considering to use to protect your data better.\n",
    "- Cynthia Dwork [\"The Definition of Differential Privacy\"](https://youtu.be/lg-VhHlztqo)\n",
    "- Chris Gilliard [\"Caught in the spotlight\"](https://urbanomnibus.net/2020/01/caught-in-the-spotlight/)\n",
    "- Richard Lai [\"WiFi mesh networks can detect your breathing\"](https://www.engadget.com/2017-10-09-origin-wireless-motion-detection-breathing-rate-sensor.html?)\n",
    "    - [Fiction short story from Ken Liu on the same topic available here](https://www.dropbox.com/s/9dd19r172sk4m9a/Forever%20Magazine%20Issue%201.epub?dl=0) (I really recommend short stories from this author!)\n",
    "- Glenn Greenwald [\"Why privacy matters\"](https://youtu.be/pcSlowAhvUk)\n",
    "- Rachel Thomas [\"Privacy and surveillance\"](https://www.youtube.com/watch?v=HwfeDeqbmsI&)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "rubric={mechanics:20}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<p>You receive marks for submitting your lab correctly, please follow these instructions:</p>\n",
    "\n",
    "<ul>\n",
    "  <li><a href=\"https://ubc-mds.github.io/resources_pages/general_lab_instructions/\">\n",
    "      Follow the general lab instructions.</a></li>\n",
    "  <li><a href=\"https://github.com/UBC-MDS/public/tree/master/rubric\">\n",
    "      Click here to view a description of the rubrics used to grade the questions</a></li>\n",
    "  <li>Push your <code>.ipynb</code> file to your GitHub repository for this lab (make at least three commits).</li>\n",
    "  <li>Upload your <code>.ipynb</code> file to Gradescope.\n",
    "  </li>\n",
    "  <li>Include a clickable link to your GitHub repo for the lab just below this cell\n",
    "    <ul>\n",
    "      <li>It should look something like this https://github.ubc.ca/MDS-2022-23/DSCI_541_labX_yourcwl.</li>\n",
    "      <li>If you are working in a group, you can create you own (public) repo in <a href=\"https://github.ubc.ca/MDS-2023-24\"> the UBC-MDS organization</a> and link that instead.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "<li>All your written answers must be in your own words.</li>\n",
    "<li>You are not allowed to use generative AI tools to write your answers for you or simply paraphrase answer that you generate from these tools (that will lead to a failing grade), but you can use them to further understand the topics you are learning about.</li>\n",
    " \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/riyaeliza123/DSCI_541_lab2_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall writing quality\n",
    "rubric={writing:20}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<p>You will receive an overall writing grade for the entire lab instead of for each question. This is just a small part of your total grade, but please use the Jupyter Lab spell checker extension to catch typos and read through your text for grammatical errors before submitting (or paste it into Google Docs/MS Word/Grammarly. You don't need to type anything under this cell, it is just a placeholder to generate the grading rubric.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Short answer questions\n",
    "\n",
    "Keep your replies brief, 1-3 sentences per question. Although these are short answer questions, don't copy answers from the readings, use your own words so that you practice learning these concepts. These will not be discussed during the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1\n",
    "rubric={reasoning:60}\n",
    "    \n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<ol type=\"1\">\n",
    "<li>What is a data broker?</li>\n",
    "<li>What are direct and indirect identifiers?</li>\n",
    "<li>What is k-anonymity and l-diversity?</li>\n",
    "<li>What are some weaknesses of k-anonymity (and l-diversity)? How could you re-identify individuals in these datasets?</li>\n",
    "<li>If you have two differentially private datasets, one with and one without your data, what does differential privacy guarantee regarding your privacy?</li>\n",
    "<li>How does differential privacy work on a conceptual level (watching the optional video from Minute Physics can help with this if you want more details and examples than what was given in the lecture)?</li>\n",
    "<li>What are some additional approaches you could take to secure data stored in your organization?</li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A data broker is a company or individual that collects, processes, and sells data about individuals. They gather data from various sources, such as public records, online activities, surveys etc. which is then analyzed, organized, and packaged into profiles or datasets that can be valuable targeted advertising, market research and so on.\n",
    "\n",
    "2. Direct identifiers are pieces of information that explicitly identify an individual. Indirect identifiers are pieces of information that, on their own, may not identify an individual directly, but when combined with other data or used in a certain context, they can lead to the identification of a person. Examples of direct identifiers are name, email and SSN, while indirect identifiers can be zipcode, gender, salary range and so on.\n",
    "\n",
    "3. K-anonymity and L-diversity concepts are introduced to bring anonymity into a certain dataset that is curated, in order to protect a participant's privacy. K-anonymity requires the dataset to have atleaset K individuals that are indistinguishable from each other with respect to certain attributes. L-diversity is an extension of k-anonymity that addresses the limitation of k-anonymity by ensuring not only indistinguishability but also diversity in the sensitive attributes of the anonymized records.\n",
    "\n",
    "4. Even though k-anonymity can help to anonymize the dataset, it does have some drawbacks. For example if the attacker is an ex-employee of the company they attack, they might have enough background information to decipher the anonymized data using their understanding of the company structure and their knowledge of the employees. Another possible attack is if a group of individuals sharing the same quasi-identifiers all have the same value for a sensitive attribute, it reduces the protection level.\n",
    "\n",
    "5. The fundamental guarantee of differential privacy is that the presence or absence of any your data (an individual's data) should not significantly impact the output or analysis of the dataset. This means that the results derived should be general and not specific to the individuals, that is, the analysis must not compromise the anonymity of any individual in the dataset.\n",
    "\n",
    "6. Differential privacy works by adding carefully calibrated noise to thr analyses performed on a dataset. The goal is to ensure that the inclusion or exclusion of any individual's data does not have a significant impact on the output, making it challenging for an external observer to discern specific information about any particular individual\n",
    "\n",
    "7. Some additional approaches we can take to ensure data privacy are data encryption, regular audits and monitoring, data masking and endpoint security measures being employed.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Discussion questions\n",
    "\n",
    "This section asks you to expand a bit on your reasoning, but still aim to write succinct replies around one paragraph per sub-question. The goal of lab discussions are not to provide you with the right answers, but to help your discussion along. Your TA will assist in this by bringing up topics that you might not have thought of, ask questions to break the silence or a dead end, and move the conversation along so that you have time to go through most questions. How useful the lab discussion is for your submission ultimately relies on that you actively contribute to the discussion and help your peers contribute and exchange ideas.\n",
    "\n",
    "##  Some tips to make your discussions in lab more effective\n",
    "\n",
    "It is easy to overlook the flaws of our own reasoning,\n",
    "so having a discussion with colleagues is an excellent opportunity\n",
    "to develop your thinking and receive feedback\n",
    "from someone who can provide an alternative perspective from your own.\n",
    "Nevertheless,\n",
    "many people don't know how to have an effective discussion,\n",
    "so I am sharing a few tips for you to be able to make the most out of this opportunity:\n",
    "\n",
    "- Commit to learning, not \"winning\" debates. \n",
    "- Comment in order to share information and develop arguments further, not to persuade.\n",
    "- Listen respectfully, without interrupting, to try to understand each others' views.\n",
    "    - Don't focus on what you are going to say next while someone else is talking.\n",
    "- Challenge ideas, not individuals.\n",
    "    - And be open to having your own ideas challenged.\n",
    "- Think about as good arguments as possible against your position.\n",
    "    - This is especially useful if many of your peers have the same opinion, help your group find angles that you might otherwise be missing.\n",
    "- Allow everyone the chance to speak.\n",
    "    - Politely ask members of your group about their opinion.\n",
    "- Avoid assumptions about any member of the class or generalizations about social groups.\n",
    "    - Be careful about asking individuals to speak on the behalf of their (perceived) social group.\n",
    "- Be aware of [logical fallacies](https://blog.hubspot.com/marketing/common-logical-fallacies), but avoid pointing them out in rude or disrespectful ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 -- Technology and privacy\n",
    "rubric={reasoning:100}\n",
    " \n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<ol type=\"1\">\n",
    "\n",
    "<li><b>Acceptable exchange.</b> Do you consider it acceptable that companies collect information about your online behavior in exchange for using their services? Is the service they provide a reasonable compensation of your data or do you think they owe you monetary compensation if they profit from selling your data to a third party? Include an example of a service you think provides a reasonable or unreasonable trade off.</li>\n",
    "<li><b>Pay for privacy.</b> If you had the option would you pay to use email, social media, file storage, etc that met your preferred privacy standard? Are there specific online services which you would be more likely to pay for than others (and why do you think privacy is more important in these cases)? Is privacy a commodity that should only be available to those who can afford it, or is it a right that should be regulated and available to all for free?</li>\n",
    "\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1.**\n",
    "\n",
    "- When companies collect information about our online behavior then the users are mainly concerned with privacy and the potential misuse of their data. The problem further arises when the companies are not transparent with their data collection policies and don’t explicitly ask for user consent to collect their behavioral data. For example, [Samsung with its Smart TVs was recording user conversations or voice recordings without transparent consent](https://www.theguardian.com/technology/2015/feb/27/samsung-voice-recording-smart-tv-breach-privacy-law-campaigners-claim) from their living rooms back in 2015. If the above two concerns regarding privacy and consent are protected by the companies, then we think it's still an acceptable exchange and we feel it's similar to the concept of Social Contract Theory. Also, the governments can help regulate this exchange by passing stringent laws that protect the customer as done by the European Union in its GDPR regulation.\n",
    "\n",
    "- We feel that the very basic services an online company provides, for example Facebook allowing us to use Instagram for free, should be the minimum compensation for the exchange of data. It’s similar to a barter system where anonymized user information is exchanged for using the platform. For further user’s compensation, it should depend on the company's business model whether they want to pay back the users for this information share or not. The local governments should also pass laws preventing disproportionate profit minting. More than compensation the users privacy and consent is more important and that should be protected as a non-negotiable.\n",
    "\n",
    "- **Reasonable tradeoff**: Google’s suite of services such as Gmail, Google Search, Google Maps are the services which are offered for free to users, and in exchange Google collects data to improve its search results, offer location based services etc. The value derived from these services, including efficient email management, accurate search results, and convenient navigation, is often seen as a fair exchange for the data shared by users.\n",
    "\n",
    "- **Unreasonable tradeoff**: [TikTok has been in controversy for its data collection practices](https://www.theguardian.com/technology/2022/jul/19/tiktok-has-been-accused-of-aggressive-data-harvesting-is-your-information-at-risk). TikTok’s data collection methods include the ability to collect user contact lists, access calendars, scan hard drives including external ones and geo locate devices on an hourly basis. While using the app it requires significantly more permissions than it needs. We feel that the value derived from the platform doesn’t justify the risks associated with the extent of data collection and privacy concerns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2**\n",
    "\n",
    "- We think that ensuring basic privacy for everyone is important but providing additional features at a cost could help sustain free services while offering premium options to customers who value heightened privacy. Further, there is a significant market for competitors and alternatives who focus more on privacy like ProtonMail for email instead of Gmail, Signal for messaging instead of WhatsApp, and privacy-focused cloud storage services like Tresorit. Another good example of the case that the customers will pay extra if the company offers better privacy and security features is Apple. Apple phones are usually more expensive than the competitors but it has currently 60% of Smartphone market share in the United States as of 2024, which shows customers are willing to pay more for better privacy.\n",
    "\n",
    "- Cloud Storage and File Sharing services, Communication and Messaging Services, and Password Managers are among the instances where individuals may find themselves willing to invest more than in other areas. Users frequently opt for premium cloud storage services that prioritize robust security and privacy features, a vital consideration for both individuals and businesses managing sensitive or confidential data. Password managers play a crucial role in storing and encrypting sensitive login information. Many users gravitate towards paid services that provide advanced security features, including two-factor authentication and secure password sharing, to fortify their accounts against unauthorized access. On the other hand we think services like social media which are associated with freedom of speech should be freely available to all.\n",
    "\n",
    "- Basic privacy should be a fundamental right that should be protected and available to everyone regardless of their financial means. Companies can charge on heightened security and privacy features. The right to privacy is recognized in various international human rights documents, such as the Universal Declaration of Human Rights and the International Covenant on Civil and Political Rights. Treating privacy as a total commodity raises ethical concerns as it will lead to more societal inequalities and compromise the principles of fairness. For example, an uneducated small scale farmer won’t be able to protect himself and his family if that happens. Let’s say his health data is sold off to insurance companies without anonymized data protection, then the insurance companies can misuse that to charge him a higher premium leading to a more unequal society. Hence, the right to basic privacy is a fundamental right for all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2 -- Technology and privacy\n",
    "rubric={reasoning:100}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<ol>\n",
    "<li><b>Terms of service.</b> When we sign up for online services, we often have to agree to terms of conditions to use the services. Sometimes these do indeed mention that your data is being collected by the company, but studies have shown that the language used in terms and conditions often requires college level reading apprehension, while many users of such platforms are children in middle and high school. Could you think of some ways to improve the presentation of terms of services to convey the information more effectively?</li>\n",
    "<li><b>Alternative options.</b> A common argument is that we can just opt out of services we don't like and use others instead (i.e. it is the indvidual's responsibility), but is this really true? Or are we getting to the stage where we \"need\" certain online services for us to integrate in society and opting out is not really an option? For example, is there a widely used replacement alternative for services such as Facebook, Instagram, YouTube or TikTok that does not center their business model on mininig user information?</li>\n",
    "<li><b>Beyond online tracking.</b> What about extending the data collection outside the online environment? Which of the following do you find reasonable and unreasonable privacy-wise? Are these examples different from tracking online (why/why not)?\n",
    "<ul>\n",
    "<li>Is it ok if an Amazon Echo/Google Home device records your voice at home?</li>\n",
    "<li>What if you have friends over, do you need to inform them about your echo device and should their voices be recorded too?</li>\n",
    "<li>What about a pair of glasses that automatically record video and sound as you walk around in public places?</li>\n",
    "<li>Or using WiFi signals to detect people moving in their homes?</li>\n",
    "</ul></li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Terms and conditions set by any company should be understandable and user friendly, so that it reaches to all age groups of people. Though they are legal documents and must thus understandably include some legalese, the legal language used in such documents is often not understandable to lay readers, even if they do possess college level reading abilities, which in itself is an incredibly high bar, not just because of the high school and middle school students using these platforms, but also general consumers who might not have a college education, or might not be fluent in the language used to write the terms and services document. To expect that consumers should be able to read and understand the entirety of most terms and services is thus an unreasonable standard, and might even carry with it the implication that only consumers who are highly educated deserve to be fully informed regarding what they agree to when using such online services. As we will discuss below, this is unfair, especially since in the modern era, many of us, including those who might not have this college level reading ability, often have no choice but to use online services, even if we might not understand the terms and services.\n",
    "\n",
    "We suggest few ways to improve the quality of terms of service by companies. Firstly, companies can write the terms and conditions in a simple and short form using simple terms instead of complicated language, or at least provide users with this simply written document to explain the complex legal document. By doing so, it will be easy for middle and high school children, as well as anyone who does not have a college level reading ability, to understand how their data is being is used and what are the conditions regarding using the services are. Secondly, providing key points or highlighting them will helps users to focus on them and get a clear understanding. Additionally, instead of displaying all the terms and conditions at a time as a long and intimidating document, revealing the points in digestable chunks will help users for better understanding the content. Lastly, using interactive elements, like using audio or graphical elements to explain the terms and conditions, might attract the attention of users and facilitate their understanding of the document. An enhancement for this can be, based on their location, the audio can be generated/translated on user's language. So by making these improvements, the terms of service can be conveyed effectively to all age groups. To ensure that companies do not make use of these simplified documents to conceal information from users, third parties can be involved to audit not only companies' terms and services documents, but also the execution of how they inform users about the terms and services.\n",
    "\n",
    "2. In this modern era, digital tools have become not only ubiquitous, but also inseparable from modern living. As many countries move to become cashless societies, or more broadly, 'smart nations', many essential financial and governmental services have been moved to online platforms - take, for instance, online banking, or accessing government services, which often mandate users to download certain software applications and agree to their conditions before being able to use these essential services. For example, in applying for our student permits to come to Canada, we were essentially forced to download and use Adobe Reader to download and fill up the documents required. It is thus tough to argue that it is feasible for us to opt out of services we do not like in favour of others, and even tougher to argue that if an individual is uncomfortable with the internet, they can opt not to use it. \n",
    "\n",
    "Beyond these services, many non-governmental but equally crucial aspects of life like communication, information sharing, payments, entertainment are all conducted on digital platforms like Whatsapp, Facebook, Gmail, YouTube, TikTok and many more. It is also difficult to opt out from these services as they are also linked to our day to day activities. Many of these companies have near monopolies over their markets, and people are very hesitant to move to other alternative options. This is as many of these services depend on having a critical mass of people we know using these services to work - for example, it would be hard to communicate with our loved ones on a messenging platform which has better terms and conditions and does not mine user information for profit if no one we know is willing to move to the alternative platform. One instance demonstrating this problem is the updating of Whatsapp's privacy policy during the COVID-19 pandemic. Whatsapp was bought by Meta around that time, resulting in updates to its privacy policy, which (as we understand) included changes like sharing some user data to Meta. This change concerned many people regarding how their personal data is being shared and what is being shared. Spurred by this change, at that point, many people shifted to use other communication app Signal which is a privacy-focused messaging app with end-to-end encryption. However, as the majority of people did not migrate to Signal from Whatsapp, this change did not last, and many people switched back to Whatsapp (though to Whatsapp's credit, they also addressed some of the concerns raised which might also have won some customers back). This demonstrates how there is often no 'true' alternative for the many online services which we use to carry out essential daily functions, even if other apps/services which do not mine user information might exist.\n",
    "\n",
    "3. Extending data collection outside the online environment can include voice assistants, wearable technology, smart home devices, etc., collecting data in the physical world.\n",
    "    - Voice Recording Devices at Home: This raises privacy concerns, especially regarding the extent of data collection and potential eavesdropping.\n",
    "    - Informing Guests about Devices: Ethically, it's advisable to inform guests about active recording devices.\n",
    "Recording Glasses in Public: This could be invasive and ethically questionable as it involves recording others without consent.\n",
    "    - WiFi Signal Tracking: Using WiFi to detect movement in homes can be seen as intrusive and raises significant privacy concerns.\n",
    "\n",
    "    We think each of these examples shares the common issue of consent and the balance between technological benefits and privacy rights and they are similar to online tracking. We are uncomfortable with these examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3 -- Scraping data and facial recognition software\n",
    "rubric={reasoning:100}\n",
    "    \n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<ol type=\"1\">\n",
    "<li><b>Facilitating queries on scraped data.</b> Is it fair game to scrape public information from the internet, and then facilitate queries on that information or are there any additional concerns when the data becomes easier to access? What about scraping data that is only public for anyone logged in to that platform but otherwise not accessible (e.g. Facebook or OKCupid) and then making it available outside the platform?</li>\n",
    "<li><b>Privacy and control of scraped data.</b> Clearview AI is a company that provides facial recognition software, which is used by private companies, law enforcement agencies, universities and individuals. The database consists of more than three billion images scraped from the internet, including from social media applications. Clearview claims 99% accuracy for most photos and one of their goals is to find criminals more accurately than current approaches used by law enforcement. One of its biggest technical advantages over previous law enforcement tools is that they have a huge set of images including the general public, not just photos of previous convicts. <a href=\"https://openmedia.org/press/item/privacy-commissioners-find-police-use-of-clearview-ai-violated-privacy-rights-of-canadians\">A recent data breach revealed that Clearview AI is employed among American and Canadian law enforcement, including by the Vancouver Police Department</a>.\n",
    "<ul>\n",
    "<li>How do you think such powerful facial recognition technology should be regulated? For example, is it fair use to scrape data that was public at the time of scraping and store it permanently in a database or do we have a <a href=\"https://en.wikipedia.org/wiki/Right_to_be_forgotten\">a right to be forgotten</a> even in downloaded data? Should we take into consideration that they are using this data for a good cause, such as catching criminals (e.g. Clearview is claimed to have been used to identify participants in the Capitol hill riot and to identify dead soldiers in the Russia/Ukraine war)? Also think about how the scale of implementation matters, even for algorithms that are 99% accurate.</li>\n",
    "</ul></li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Overall, our group feels that it is alright to scrape public information, especially if this is done on a personal, non-commercial basis. However, we feel that it is not fair game in the second instance, where data that is public in one platform is scraped and shared to another platform. It must be acknowledged that with the advent of the digital age, the lines between the public and private spheres have become more blurred. Furthermore, in the absence of widespread, global agreements on ethical conduct on the internet or the handling of data by online platforms, the terms of the social contracts we agree to in using the internet have become more and more implicit and murky. That said, we feel that, in the spirit of individual responsibility, users should understand that information that they choose to make publicly available on the internet is in the public sphere, and is thus fair game for scraping. However, in the second instance, we feel that the users' choice to only make their data public on the platform given the restrictions on the platform is a clear indication that the data they put there is not entirely in the public sphere. Scraping data from there and making it available outside the platform is hence a violation of their privacy, at least in our opinion.\n",
    "\n",
    "2. We think that, as with most powerful tools, it is clear that this facial recognition technology is a double edged sword. We find the permanent storing of user data in a database unnerving, since this means that the users essentially cede control of their data to Clearview AI, and are unable to delete their data off the Clearview AI database, even if they might have deleted it off their public platforms. It is especially disturbing as most users probably are not aware of this, and are under the impression that they have control over the images of themselves (or other data) they share publicly, so this feels like a violation of privacy. As such, we feel that users do have a right to be forgotten even in downloaded data. One solution for implementing this could be the periodic updating of the Clearview AI database, such that images which are no longer public are not downloaded. However, we understand that this is far from a perfect solution as this might cause problems regarding the training of the AI, and an inaccurate model might also bring its own consequences. For example, if an innocent person is wrongly identified as the perpetrator of a crime due to the inaccuracy of the AI as a result of this database updating, it may be argued that the societal harm caused by the potential increased errors might outweigh the societal harms of permanently storing user data (though I would argue that this merely means that less weight should be given to technological tools like Clearview AI in making decisions which might severely alter the course of a person's life - in essence, that this is more a problem that should be solved in policies regarding the use of AI tools than it is a technological problem). In addition, depending on the time between database updates, users data might still be stored and used for a longer time than users might want want. To address the issue of user control and the informed consent of users, we feel that credits should be given and permissions should be explicitly sought as much as possible when it comes to using user data.\n",
    "\n",
    "Whether or not the data was used for a good cause is a complicated issue, since the 'goodness' of a cause or its methods might differ between contexts, and even from person to person. The opportunity for misuse is thus present, even with this caveat. For example, while it might seem uncomplicated to say that using Clearview AI to solve crimes is a good cause, what constitutes a crime is not universally agreed upon, with some acts, like voicing unhappiness against the government or being of a certain race or religion, being completely acceptable in some countries but illegal in others. Furthermore, governments are not saints, and dysfunctional governments (or even functional but authoritarian-leaning governments) might weaponise and abuse their access to these services for their own purposes, for example, to persecute minority populations or political opponents. There is also an issue of what users consent to when making their data selectively public, such as by uploading their data onto Facebook. Taking the [TraceTogether](https://www.channelnewsasia.com/singapore/bill-restrict-tracetogether-to-serious-crimes-passed-parliament-297611) (also [here](https://www.straitstimes.com/singapore/politics/tracetogether-data-was-accessed-in-may-2020-for-punggol-fields-murder)) case as an example, we see that the use of user data which users might have willingly given up to the government/companies for another purpose, even if it is a good cause (in this case, a murder investigation) might cause users to feel that their privacy has been violated, and that lawmakers also have different views on what is a 'good enough' cause to justify the use of user data in a way that was not the initial intent. Similarly, users might not be willing to let their publicly accessible information be used in making a facial recognition technology which might be used to positively identify and implicate them in a crime, regardless of whether they intend to commit crimes. Thus, to argue that this is alright because the data is used for a 'good cause' is a morally grey area, and we are not comfortable with agreeing completely with this practice.\n",
    "\n",
    "As mentioned above, we think that this is not really just a technological problem, but also a policy problem. We feel that laws regarding data, and the consequences for data breaches, should be strengthened to deter misuse of such powerful technologies, not only by individuals or companies, but also by countries. While we feel that individual countries ultimately bear the responsibility of protecting their civilians against cyber crime, we recognise how the international nature of the internet might make legislation difficult. We also recognise, as mentioned above, that countries themselves might misuse these tools and user data. As such, third party organisations and international bodies should also be involved in the legislation and regulation of the use of these tools. As an aside on the scale of implementation, given the potential harms these AI tools can cause if they are used in important contexts like sentencing a person for crimes, in general, we feel that these tools should be treated with great caution, and humans should always be involved in the use of these tools, especially in situations which have potentially life altering consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.4 (Challenging)\n",
    "rubric={reasoning:20}\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "<p><em>Note: Since this exercise is a bit longer than the regular challenging questions, it will count <strong>both for the ~5% challenging points for this lab, and the 2% extra credit assignment for your overall grade in 541.</strong></em></p>\n",
    "<p>In previous years, students have asked for an exercise on how to implement differential privacy in a data analysis. It’s difficult to fit this into the curriculum because many of the differential privacy libraries are not trivial to use and would take a fair amount of time to learn properly.</p>\n",
    "<p>This challenging exercise is an experiment where I collaborated with the team behind <a href=\"https://www.antigranular.com/\"> Antigranular</a>, a community-led open-source platform that combines confidential computing with differential privacy. They have put together a notebook with an introduction to a couple of differential privacy libraries that are easier to get started with. Antigranular is aiming to develop a general curriculum for privacy concerned data analysis, so as part of this exercise you will give feedback on how easy it was to follow along in the tutorial notebook and what you think could be improved.</p>\n",
    "<p>You can <a href=\"https://github.ubc.ca/MDS-2023-24/DSCI_541_priv-eth-sec_students/blob/master/release/Differential_Privacy_A_hands_on_introduction.ipynb\">access the notebook in the student repo</a>. You can either study the output of the notebook without running it, or re-run the cells and change things around if you want to do more experimentation. You task is to read through the notebook and answer the following questions:</p>\n",
    "<ol type=\"1\">\n",
    "<li>Why is the salary that is printed for the new employee so different when using differential privacy versus when we are not using it? Would you say this is a feature of differential privacy or a drawback?\n",
    "</li>\n",
    "<li>What does it mean that there is a tradeoff between privacy and utility? Describe this in the context of differential privacy specifically referring to the figure with the same name in the notebook and comment on why the size of the data matters.</li>\n",
    "<li>Provide feedback on your general experience working through the notebook. What worked well and what do you think could be improved? Was it suitable for someone with your background level of experience with differential privacy (a brief high level introduction to the main concepts during lecture)? Did you find that seeing examples in actual code helpful for your understanding of what differential privacy is and how it can be employed in a data analysis?</li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWERS HERE\n",
    "\n",
    "1.\n",
    "\n",
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "feedback"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "# Help us improve the labs\n",
    "\n",
    "The MDS program is continually looking to improve our courses, including lab questions and content. The following optional questions will not affect your grade in any way nor will they be used for anything other than program improvement:\n",
    "\n",
    "1. Approximately how many hours did you spend working or thinking about this assignment (including lab time)?\n",
    "\n",
    "#Ans:\n",
    "\n",
    "2. Were there any questions that you particularly liked or disliked?\n",
    "\n",
    "#Ans: [Questions you liked]\n",
    "\n",
    "#Ans: [Questions you disliked]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b98692558f58bef945e1a53707f9e7cae3bda69fed90edc0c37d8470acb8e49e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
